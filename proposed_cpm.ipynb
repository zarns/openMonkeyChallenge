{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQUWcJgKtQSE"
      },
      "outputs": [],
      "source": [
        "from matplotlib.transforms import ScaledTranslation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torch_data\n",
        "import numpy as np\n",
        "import torch.nn.functional as F \n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import cv2\n",
        "import skimage.filters as filters\n",
        "import skimage.transform as transform\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "#Set device to GPU if possible\n",
        "cuda = torch.cuda.is_available()\n",
        "device = 'cuda:0' if cuda else 'cpu'\n",
        "\n",
        "#print(cuda)\n",
        "print(\"GPU availability: {}; automation will be using: {} \".format(cuda, device), end= '')\n",
        "\n",
        "#Path to where data is stored\n",
        "drive.mount('/content/gdrive/', force_remount = True)\n",
        "dir_path = \"/content/gdrive/MyDrive/Colab Notebooks/Data/\"\n",
        "\n",
        "#Set automation to Train or Test\n",
        "Train = False\n",
        "\n",
        "def Load_Data():\n",
        "\n",
        "  if(Train == True):\n",
        "    dir = os.path.join(dir_path, \"train_annotation.json\")\n",
        "  else:\n",
        "    dir = os.path.join(dir_path, \"val_annotation.json\")\n",
        "\n",
        "  with open(dir) as f:\n",
        "      dictionary = json.load(f)\n",
        "      feature_list = [item for item in dictionary['data']]\n",
        "      feature_list = np.asarray(feature_list)\n",
        "\n",
        "  return(feature_list)\n",
        "\n",
        "def get_image(feature_list, index):\n",
        "\n",
        "  if(Train == True):\n",
        "    dir_image_folder = os.path.join(dir_path, \"train\")\n",
        "  else:\n",
        "    dir_image_folder = os.path.join(dir_path, \"val\")\n",
        "    \n",
        "  # print(\"dir_image_folder: \",dir_image_folder)\n",
        "\n",
        "  feature = feature_list[index]\n",
        "\n",
        "  dir_image = os.path.join(dir_image_folder, feature[\"file\"])\n",
        "\n",
        "  img = mpimg.imread(dir_image)\n",
        "\n",
        "  #plt.imshow(img)\n",
        "\n",
        "  return img\n",
        "\n",
        "#Return batches with image filepaths. Will need to \"get_image\" by batch as\n",
        "#RAM capacity is surpassed if we attempt to get all images within the create batch\n",
        "#method.\n",
        "\n",
        "def create_batches(data, batch_size):\n",
        "\n",
        "  data_count = len(data)\n",
        "  index_array = random.sample(range(0, data_count), data_count)\n",
        "\n",
        "  mini_batch_count = math.floor(data_count/batch_size)\n",
        "\n",
        "  mini_batch_x = []\n",
        "\n",
        "  for batch in range(int(mini_batch_count)):\n",
        "    batch_x = []\n",
        "    try:\n",
        "      for value in range(batch_size):\n",
        "        batch_x.append(index_array[value + batch * batch_size])\n",
        "      batch_x = np.asarray(batch_x)\n",
        "      mini_batch_x.append(batch_x)\n",
        "    except:\n",
        "      batch_x = np.asarray(batch_x)\n",
        "      mini_batch_x.append(batch_x)\n",
        "  mini_batch_x = np.asarray(mini_batch_x)\n",
        "  return mini_batch_x\n",
        "#torch_data.dataloader(data_class, batch_size = 1, shuffle = True)\n",
        "\n",
        "def get_batch_data(data, batch, size):\n",
        "  batch_data_train_images = []\n",
        "  batch_data_heat_maps = []\n",
        "  batch_data_center_heat_map = []\n",
        "\n",
        "  heat_map_size = (32, 32)\n",
        "  center_map_size = (256, 256)\n",
        "\n",
        "  for value in range(len(batch)):\n",
        "    print(batch[value])\n",
        "    img = get_image(data, batch[value])\n",
        "    cropped_image, cropped_joints, cropped_center = crop_image(img, data[batch[value]], size)\n",
        "    #Create ground truth heatmap and center heat map\n",
        "    heat_maps, center_heat_map = get_heatmap([np.zeros(heat_map_size), np.zeros(center_map_size)], cropped_joints/8, cropped_center)\n",
        "    train_image = np.transpose(cropped_image, (2, 0, 1))/255.0\n",
        "    batch_data_train_images.append(train_image)\n",
        "    batch_data_heat_maps.append(heat_maps)\n",
        "    batch_data_center_heat_map.append(center_heat_map)\n",
        "\n",
        "  batch_data_train_images = np.asarray(batch_data_train_images)\n",
        "  batch_data_heat_maps = np.asarray(batch_data_heat_maps)\n",
        "  batch_data_center_heat_map = np.asarray(batch_data_center_heat_map)\n",
        "\n",
        "  return batch_data_train_images, batch_data_heat_maps, batch_data_center_heat_map\n",
        "\n",
        "def crop_image(img, features, image_size):\n",
        "  joint_points = features[\"landmarks\"]\n",
        "  bbox = features[\"bbox\"]\n",
        "  visibility = np.asarray(features[\"visibility\"])\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  for value in range(0, len(joint_points), 2):\n",
        "    x.append(joint_points[value])\n",
        "  width_points = np.transpose(np.asarray(x))\n",
        "\n",
        "  for value in range(1, len(joint_points), 2):\n",
        "    y.append(joint_points[value])\n",
        "  height_points = np.transpose(np.asarray(y))\n",
        "\n",
        "  Height, Width = img.shape[0], img.shape[1]\n",
        "\n",
        "  bbox_x1 = math.floor(bbox[0])\n",
        "  bbox_x2 = math.floor(bbox[0] + bbox[2])\n",
        "  bbox_y1 = math.floor(bbox[1])\n",
        "  bbox_y2 = math.floor(bbox[1] + bbox[3])\n",
        "\n",
        "  cropped_x = bbox_x2 - bbox_x1\n",
        "  cropped_y = bbox_y2 - bbox_y1\n",
        "\n",
        "  if(cropped_y > cropped_x):\n",
        "    cropped_difference = cropped_y - cropped_x\n",
        "    bbox_x1 = bbox_x1 - math.floor(cropped_difference/2)\n",
        "    bbox_x2 = bbox_x2 + math.floor(cropped_difference/2)\n",
        "  elif(cropped_x > cropped_y):\n",
        "    cropped_difference = cropped_x - cropped_y\n",
        "    bbox_y1 = bbox_y1 - math.floor(cropped_difference/2)\n",
        "    bbox_y2 = bbox_y2 + math.floor(cropped_difference/2)\n",
        "\n",
        "  #Sift our Width and Height points for any zero values\n",
        "  width_points = np.where(width_points != 0, width_points - bbox_x1, width_points)\n",
        "  height_points = np.where(height_points != 0, height_points - bbox_y1, height_points)\n",
        "\n",
        "  #Determine if padding is needed for image\n",
        "  padding = 0\n",
        "\n",
        "  if((bbox_y1 < 0) or (bbox_x1 < 0) or (bbox_y2 > Height) or (bbox_x2 > Width)):\n",
        "    padding = math.floor(max(-bbox_y1, -bbox_x1, bbox_y2 - Height, bbox_x2 - Width))\n",
        "    img = np.pad(img, ((padding, padding), (padding, padding), (0, 0)))\n",
        "\n",
        "  img = img[bbox_y1 + padding : bbox_y2 + padding, bbox_x1 + padding : bbox_x2 + padding]\n",
        "\n",
        "  bbox[0] = bbox[0] - bbox_x1\n",
        "  bbox[1] = bbox[1] - bbox_y1\n",
        "\n",
        "  #Resize our image\n",
        "  Height, Width = img.shape[0], img.shape[1]\n",
        "  width_points = (width_points * image_size)/Width\n",
        "  height_points = (height_points * image_size)/Width\n",
        "\n",
        "  center = np.array((1, 2))\n",
        "\n",
        "  center[0] = (abs(math.floor((bbox_x2 - bbox_x1)/2))) * (image_size/Width)\n",
        "  center[1] = (abs(math.floor((bbox_y2 - bbox_y1)/2))) * (image_size/Height)\n",
        "\n",
        "  img = cv2.resize(img, (image_size, image_size))\n",
        "\n",
        "  joint_points = [[width_points[i], height_points[i]] for i in range(len(height_points))]\n",
        "\n",
        "  return img, np.asarray(joint_points), np.asarray(center)\n",
        "\n",
        "def get_heatmap(img, joint_points, cropped_center):\n",
        "  heat_map_size = img[0]\n",
        "  #print(heat_map_size.shape)\n",
        "  Height, Width = heat_map_size.shape[0], heat_map_size.shape[1]\n",
        "  #print(joint_points.shape)\n",
        "\n",
        "  #Build shell for heatmap of each joint_point\n",
        "  heat_maps = np.zeros((Height, Width, (len(joint_points) + 1)))\n",
        "\n",
        "  for joint in range(len(joint_points)):\n",
        "    if((joint_points[joint, 0] == 0) and (joint_points[joint, 1] == 0)):\n",
        "      continue\n",
        "\n",
        "    if(joint_points[joint, 1] >= Height):\n",
        "      joint_points[joint, 1] = Height - 1\n",
        "    if(joint_points[joint, 0] >= Width):\n",
        "      joint_points[joint, 0] = Width - 1\n",
        "    \n",
        "    #Build heatmap of joint\n",
        "    heat_map = heat_maps[:, :, joint]\n",
        "    x = math.floor(joint_points[joint, 1])\n",
        "    y = math.floor(joint_points[joint, 0])\n",
        "\n",
        "    #Set heatmap of joint within heatmap image\n",
        "    heat_map[x][y] = 1\n",
        "    heat_map = filters.gaussian(heat_map, sigma = 2)\n",
        "\n",
        "    #scale image to [0, 1]\n",
        "    scale = np.max(heat_map)\n",
        "    heat_map = heat_map/scale\n",
        "    heat_maps[:, :, joint] = heat_map\n",
        "\n",
        "  #add background dimensions\n",
        "  heat_maps[:, :, len(joint_points)] = 1 - np.max(heat_maps[:, :, :len(joint_points)], axis = 2)\n",
        "  heat_maps = np.transpose(heat_maps, (2, 0, 1))\n",
        "\n",
        "\n",
        "  #Center image\n",
        "  center_heat_map = img[1] #256x256\n",
        "\n",
        "  Height, Width = center_heat_map.shape[0], center_heat_map.shape[1]\n",
        "  x = math.floor(cropped_center[1]); y = math.floor(cropped_center[0])\n",
        "  center_heat_map[x][y] = 1\n",
        "\n",
        "  center_heat_map = filters.gaussian(center_heat_map, sigma = 2)\n",
        "  scale = np.max(center_heat_map)\n",
        "  center_heat_map = center_heat_map/scale\n",
        "  center_heat_map = np.expand_dims(center_heat_map, axis = 0)\n",
        "  #print(center_heat_map.shape)\n",
        "  return heat_maps, center_heat_map\n",
        "\n",
        "def view_heat_maps(img, heat_maps):\n",
        "  Height, Width = img.shape[0], img.shape[1]\n",
        "\n",
        "  heat_maps = np.transpose(heat_maps)\n",
        "\n",
        "  dictionary_of_names = {0: 'Image', 1: 'Right Eye', 2: 'Left Eye', 3: 'Nose', 4: 'Head',\n",
        "    5: 'Neck', 6: 'Right Shoulder', 7: 'Right Elbow', 8: 'Right Wrist', 9: 'Left Shoulder',\n",
        "    10: 'Left Elbow', 11: 'Left Wrist', 12: 'Hip', 13: 'Right Knee', 14: 'Right Ankle',\n",
        "    15: 'Left Knee', 16: 'Left Ankle', 17: 'Tail'}\n",
        "\n",
        "  #print(heat_maps.shape)\n",
        "  if(heat_maps.shape[0] != Height):\n",
        "    heat_maps = transform.resize(heat_maps, (Height, Width))\n",
        "  elif(heat_maps.shape[1] != Width):\n",
        "    heat_maps = transform.resize(heat_maps, (Height, Width))\n",
        "\n",
        "  for value in range((heat_maps.shape[2])): #256x256x18\n",
        "    #print(value)\n",
        "    plt.subplot(4, 5, value + 1)\n",
        "    plt.title(dictionary_of_names[value], fontdict = {\"fontsize\" : 12})\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    if value == 0:\n",
        "      plt.imshow(img)\n",
        "    else:\n",
        "      plt.imshow(heat_maps[:, :, value - 1])\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVcJHyG7FmSR"
      },
      "outputs": [],
      "source": [
        "def crop_image_test(img, features, image_size): #crop_image function for testing-relevant features\n",
        "\n",
        "  bbox = features[\"bbox\"]\n",
        "  bb = bbox.copy()\n",
        "\n",
        "  Height, Width = img.shape[0], img.shape[1]\n",
        "\n",
        "  bbox_x1 = math.floor(bbox[0])\n",
        "  bbox_x2 = math.floor(bbox[0] + bbox[2])\n",
        "  bbox_y1 = math.floor(bbox[1])\n",
        "  bbox_y2 = math.floor(bbox[1] + bbox[3])\n",
        "\n",
        "  cropped_x = bbox_x2 - bbox_x1\n",
        "  cropped_y = bbox_y2 - bbox_y1\n",
        "\n",
        "  if(cropped_y > cropped_x):\n",
        "    cropped_difference = cropped_y - cropped_x\n",
        "    bbox_x1 = bbox_x1 - math.floor(cropped_difference/2)\n",
        "    bbox_x2 = bbox_x2 + math.floor(cropped_difference/2)\n",
        "  elif(cropped_x > cropped_y):\n",
        "    cropped_difference = cropped_x - cropped_y\n",
        "    bbox_y1 = bbox_y1 - math.floor(cropped_difference/2)\n",
        "    bbox_y2 = bbox_y2 + math.floor(cropped_difference/2)\n",
        "\n",
        "  #Determine if padding is needed for image\n",
        "  padding = 0\n",
        "\n",
        "  if((bbox_y1 < 0) or (bbox_x1 < 0) or (bbox_y2 > Height) or (bbox_x2 > Width)):\n",
        "    padding = math.floor(max(-bbox_y1, -bbox_x1, bbox_y2 - Height, bbox_x2 - Width))\n",
        "    img = np.pad(img, ((padding, padding), (padding, padding), (0, 0)))\n",
        "\n",
        "  img = img[bbox_y1 + padding : bbox_y2 + padding, bbox_x1 + padding : bbox_x2 + padding]\n",
        "\n",
        "  bbox[0] = bbox[0] - bbox_x1\n",
        "  bbox[1] = bbox[1] - bbox_y1\n",
        "\n",
        "  #Resize our image\n",
        "  Height, Width = img.shape[0], img.shape[1]\n",
        "\n",
        "  center = np.array((1, 2))\n",
        "\n",
        "  center[0] = (abs(math.floor((bbox_x2 - bbox_x1)/2))) * (image_size/Width)\n",
        "  center[1] = (abs(math.floor((bbox_y2 - bbox_y1)/2))) * (image_size/Height)\n",
        "\n",
        "  img = cv2.resize(img, (image_size, image_size))\n",
        "\n",
        "  return img, np.asarray(center), bb\n",
        "\n",
        "def get_cmap(img, cropped_center):\n",
        "  #Center image\n",
        "  center_heat_map = img #256x256\n",
        "\n",
        "  Height, Width = center_heat_map.shape[0], center_heat_map.shape[1]\n",
        "  x = math.floor(cropped_center[1]); y = math.floor(cropped_center[0])\n",
        "  center_heat_map[x][y] = 1\n",
        "\n",
        "  center_heat_map = filters.gaussian(center_heat_map, sigma = 2)\n",
        "  scale = np.max(center_heat_map)\n",
        "  center_heat_map = center_heat_map/scale\n",
        "  center_heat_map = np.expand_dims(center_heat_map, axis = 0)\n",
        "  #print(center_heat_map.shape)\n",
        "  return center_heat_map\n",
        "\n",
        "def get_landmarks(pred, bb): # pred is predicted heat map, bb is bounding box from json\n",
        "  landmarks = []\n",
        "  h, w = (bb[3], bb[2])  # (h, w)\n",
        "\n",
        "  if h > w:\n",
        "    scale = h / pred.shape[1]\n",
        "    offset = np.array([0, (h-w)//2])\n",
        "  else:\n",
        "    scale = w / pred.shape[1]\n",
        "    offset = np.array([(w-h)//2, 0])\n",
        "\n",
        "  for i in range(17): #num_joints\n",
        "    y, x = np.unravel_index(np.argmax(pred[i,:,:]), (pred.shape[1], pred.shape[2]))\n",
        "\n",
        "    y = y * scale - offset[0] + bb[1]\n",
        "    x = x * scale - offset[1] + bb[0]\n",
        "    \n",
        "    landmarks.append(int(x))\n",
        "    landmarks.append(int(y))\n",
        "  \n",
        "  return landmarks\n",
        "\n",
        "class TestData(Dataset):\n",
        "    def __init__(self):\n",
        "        super(TestData, self).__init__()\n",
        "        dir = os.path.join(dir_path, 'test_prediction.json')\n",
        "        with open(dir) as f:\n",
        "            dictionary = json.load(f)\n",
        "            self.features = [item for item in dictionary['data']]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_d = self.features[idx]\n",
        "        im_dir = os.path.join(dir_path, 'test', f_d['file'])\n",
        "        im = mpimg.imread(im_dir)\n",
        "\n",
        "        crop_size = 256\n",
        "        cm_size = (256, 256)\n",
        "\n",
        "        c_im, c_cen, bb = crop_image_test(im, f_d, crop_size)\n",
        "        cmap = get_cmap(np.zeros(cm_size), c_cen)\n",
        "        c_im = np.transpose(c_im, (2, 0, 1))/255.0\n",
        "\n",
        "        return c_im, cmap, bb\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "class ValData(Dataset):\n",
        "    def __init__(self):\n",
        "        super(ValData, self).__init__()\n",
        "        dir = os.path.join(dir_path, 'val_prediction.json')\n",
        "        with open(dir) as f:\n",
        "            dictionary = json.load(f)\n",
        "            self.features = [item for item in dictionary['data']]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_d = self.features[idx]\n",
        "        im_dir = os.path.join(dir_path, 'val', f_d['file'])\n",
        "        im = mpimg.imread(im_dir)\n",
        "\n",
        "        crop_size = 256\n",
        "        cm_size = (256, 256)\n",
        "\n",
        "        c_im, c_cen, bb = crop_image_test(im, f_d, crop_size)\n",
        "        cmap = get_cmap(np.zeros(cm_size), c_cen)\n",
        "        c_im = np.transpose(c_im, (2, 0, 1))/255.0\n",
        "\n",
        "        return c_im, cmap, bb\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "def predict_test():\n",
        "  model.eval()\n",
        "  test_data = DataLoader(ValData(), batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "  dir = os.path.join(dir_path, 'val_prediction.json')\n",
        "\n",
        "  with open(dir) as f:\n",
        "    dictionary = json.load(f)\n",
        "\n",
        "  for i, (im, center, bb) in enumerate(test_data):\n",
        "    # print(bb)\n",
        "    print('batch ' + str(i + 1) + '/' + str(len(test_data)))\n",
        "    im = im.float().to(device)\n",
        "    center = center.float().to(device)\n",
        "    \n",
        "    pred = model(im, center)\n",
        "\n",
        "    for j in range(len(pred)):\n",
        "      #print(len(pred))\n",
        "      dictionary['data'][i*batch_size+j]['landmarks'] = get_landmarks(pred[j,-1,:,:,:].cpu().detach().numpy(), [bb[0][j],bb[1][j],bb[2][j],bb[3][j]])\n",
        "\n",
        "    with open(dir, 'w') as out:\n",
        "      json.dump(dictionary, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdTLnjTkiNtV"
      },
      "outputs": [],
      "source": [
        "predict_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzL6nIhQjMjT"
      },
      "source": [
        "# Proposed CPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Nh5ReOOP6g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torch_data\n",
        "import numpy as np\n",
        "import torch.nn.functional as F \n",
        "\n",
        "class ProposedCPM(nn.Module): # CPM model inherits nn.Module: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "  def __init__(self, num_joints):\n",
        "    super(ProposedCPM, self).__init__()\n",
        "    self.num_joints = num_joints\n",
        "\n",
        "    self.cpool = nn.AvgPool2d(4)                               # avg pool operation with kernel size 4 for center_image\n",
        "\n",
        "    # To extract features (x) for every stage\n",
        "    self.x_conv1 = nn.Conv2d(3, 128, 9, padding=4)             # (input channels, output channels, kernel size, padding) // output channels = number of kernels used on image, might be good to play around with values\n",
        "    self.x_pool1 = nn.MaxPool2d(2)                             # max pool operation with kernel size 2\n",
        "    self.x_conv2 = nn.Conv2d(128, 128, 9, padding=4)\n",
        "    self.x_pool2 = nn.MaxPool2d(2)                 \n",
        "    self.x_conv3 = nn.Conv2d(128, 128, 9, padding=4)\n",
        "    self.x_pool3 = nn.MaxPool2d(2)\n",
        "    self.x_conv4 = nn.Conv2d(128, 128, 5, padding=2)\n",
        "    self.x_upcv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "    self.x_conv5 = nn.Conv2d(192, 32, 3, padding=1)\n",
        "    self.x_bnorm = nn.BatchNorm2d(32)\n",
        "    # self.x_upcv2 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
        "    # self.x_conv6 = nn.Conv2d(160, 32, 3, padding=1)  \n",
        "\n",
        "    # Stage 1 convolutions:\n",
        "    self.s1_conv1 = nn.Conv2d(32, 512, 9, padding=4)\n",
        "    self.s1_conv2 = nn.Conv2d(512, 512, 1)\n",
        "    self.s1_conv3 = nn.Conv2d(512, num_joints+1, 1)\n",
        "\n",
        "    # Stage >= 2 (t) convolutions:\n",
        "    self.st_conv1 = nn.Conv2d(34 + num_joints, 128, 11, padding=5)  # input channels = output channels from image features (x) + output channels from previous stage + 1 channel for loss\n",
        "    self.st_conv2 = nn.Conv2d(128, 128, 11, padding=5)\n",
        "    self.st_conv3 = nn.Conv2d(128, 128, 11, padding=5)\n",
        "    self.st_conv4 = nn.Conv2d(128, 128, 1)\n",
        "    self.st_conv5 = nn.Conv2d(128, num_joints + 1, 1)\n",
        "\n",
        "  def extract_features(self, im):                                   # input im shape = (N, 3, H, W) // output shape = (N, 32, H/8, W/8)    N = batch size\n",
        "    x = self.x_pool1(F.relu(self.x_conv1(im)))\n",
        "    x1 = self.x_pool2(F.relu(self.x_conv2(x)))\n",
        "    x2 = self.x_pool3(F.relu(self.x_conv3(x1)))\n",
        "    x2 = F.relu(self.x_conv4(x2))\n",
        "    x2 = self.x_upcv1(x2)\n",
        "    x2 = F.relu(self.x_bnorm(self.x_conv5(torch.cat([x1, x2], dim=1))))\n",
        "    return x2\n",
        "  \n",
        "  def stage_1(self, x):                                             # input x shape = (N, 32, H/8, W/8) // output shape = (N, num_joints + 1, H/8, W/8)\n",
        "    x = F.relu(self.s1_conv1(x))\n",
        "    x = F.relu(self.s1_conv2(x))\n",
        "    return F.relu(self.s1_conv3(x))\n",
        "\n",
        "  def stage_t(self, x):                                             # input x shape = (N, 34 + num_joints, H/8, W/8) // output shape = (N, num_joints + 1, H/8, W/8)\n",
        "    x = F.relu(self.st_conv1(x))\n",
        "    x = F.relu(self.st_conv2(x))\n",
        "    x = F.relu(self.st_conv3(x))\n",
        "    x = F.relu(self.st_conv4(x))\n",
        "    return F.relu(self.st_conv5(x))\n",
        "  \n",
        "  def forward(self, im, center_image): # for 6 stages\n",
        "    cpool = self.cpool(center_image)\n",
        "\n",
        "    s1_maps = self.stage_1(self.extract_features(im))\n",
        "    x = self.extract_features(im)\n",
        "    s2_maps = self.stage_t(torch.cat([x, s1_maps, cpool], dim=1))\n",
        "    s3_maps = self.stage_t(torch.cat([x, s2_maps, cpool], dim=1))\n",
        "    s4_maps = self.stage_t(torch.cat([x, s3_maps, cpool], dim=1))\n",
        "    s5_maps = self.stage_t(torch.cat([x, s4_maps, cpool], dim=1))\n",
        "    s6_maps = self.stage_t(torch.cat([x, s5_maps, cpool], dim=1))\n",
        "\n",
        "    return torch.stack([s1_maps, s2_maps, s3_maps, s4_maps, s5_maps, s6_maps], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aiscDxsykEZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TrainDataProp(Dataset):\n",
        "    def __init__(self):\n",
        "        super(TrainDataProp, self).__init__()\n",
        "        dir = os.path.join(dir_path, 'train_annotation.json')\n",
        "        with open(dir) as f:\n",
        "            dictionary = json.load(f)\n",
        "            self.features = [item for item in dictionary['data']]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_d = self.features[idx]\n",
        "        im_dir = os.path.join(dir_path, 'train', f_d['file'])\n",
        "        im = mpimg.imread(im_dir)\n",
        "\n",
        "        crop_size = 256\n",
        "        hm_size = (64, 64)\n",
        "        cm_size = (256, 256)\n",
        "\n",
        "        c_im, c_jts, c_cen = crop_image(im, f_d, crop_size)\n",
        "        hmaps, cmap = get_heatmap([np.zeros(hm_size), np.zeros(cm_size)], c_jts/4, c_cen)\n",
        "        c_im = np.transpose(c_im, (2, 0, 1))/255.0\n",
        "\n",
        "        return c_im, hmaps, cmap\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqW0tYcor52U"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = 'cuda:0' if cuda else 'cpu'\n",
        "dir = os.path.join(dir_path, \"cpm_proposed.pth\")\n",
        "\n",
        "epochs = 500\n",
        "lr = 0.0001\n",
        "batch_size = 24\n",
        "\n",
        "def train():\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  loss_array = []\n",
        "  best_loss = 0.0021\n",
        "  model.train()\n",
        "\n",
        "  train_data = DataLoader(TrainDataProp(), batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    if(epoch % 2 == 0):\n",
        "      dir = os.path.join(dir_path, \"cpm_proposed.pth\")\n",
        "      torch.save(model.state_dict(), dir)\n",
        "      print(\"Saving Model.\")\n",
        "    for i, (im, heat, center) in enumerate(train_data):\n",
        "      \n",
        "      im = im.float().to(device)\n",
        "      heat = torch.stack([heat]*6, dim=1) # stack one set of heatmaps for each stage of our CPM\n",
        "      heat = heat.float().to(device)\n",
        "      center = center.float().to(device)\n",
        "\n",
        "      pred = model(im, center)\n",
        "      loss = criterion(pred, heat)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      print('----------------- Epoch ' + str(epoch + 1) + ', batch ' + str(i + 1) + '/' + str(len(train_data)) + ', Loss: ' + str(loss.item()) + ' -----------------')\n",
        "      if loss.item() < best_loss:\n",
        "        dir = os.path.join(dir_path, \"cpm_proposed_best.pth\")\n",
        "        best_loss = loss.item()\n",
        "        torch.save(model.state_dict(), dir)\n",
        "        print('----------------- Saving Best Loss CPM ----------------------')\n",
        "\n",
        "    loss_array.append(loss)\n",
        "    print(loss)\n",
        "    \n",
        "  #Save model one final time\n",
        "  dir = os.path.join(dir_path, \"cpm_proposed.pth\")\n",
        "  torch.save(model.state_dict(), dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ProposedCPM(17).to(device)"
      ],
      "metadata": {
        "id": "T4NaRVGgvAE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvfB8rV4thvQ"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMLdkajGXEvw",
        "outputId": "de58d7ed-e16d-4030-cf9c-03662abf84bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProposedCPM(\n",
              "  (cpool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
              "  (x_conv1): Conv2d(3, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              "  (x_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (x_conv2): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              "  (x_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (x_conv3): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              "  (x_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (x_conv4): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (x_upcv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (x_conv5): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (x_bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (s1_conv1): Conv2d(32, 512, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              "  (s1_conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (s1_conv3): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (st_conv1): Conv2d(51, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
              "  (st_conv2): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
              "  (st_conv3): Conv2d(128, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
              "  (st_conv4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (st_conv5): Conv2d(128, 18, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "dir = os.path.join(dir_path, \"cpm_proposed.pth\")\n",
        "model.load_state_dict(torch.load(dir))\n",
        "dir = os.path.join(dir_path, \"cpm_proposed.pth\")\n",
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}